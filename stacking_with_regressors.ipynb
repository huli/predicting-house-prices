{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking with multiple regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import helper_functions\n",
    "import pandas as pd\n",
    "importlib.reload(helper_functions)\n",
    "from helper_functions import *\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, _transform_one\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.ensemble import BaggingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining pipeline as in other examples\n",
    "trans_pipeline = Pipeline([\n",
    "    ('impute_numerical', DFTransform(lambda X: fill_numerical_nans(X))),\n",
    "    ('impute_categorical', DFTransform(lambda X: impute_categorical(X))),\n",
    "    ('impute_special_cases', DFTransform(lambda X: impute_special_cases(X))),\n",
    "    ('ordinal_features', DFTransform(lambda X: encode_ordinals(X))),\n",
    "    ('encode_dummies', DFTransform(lambda X: create_dummies(X))),\n",
    "    ('check_nans', DFTransform(lambda X: check_nans(X))),\n",
    "    ('create_sellingage', DFTransform(lambda X: create_sellingage(X))),\n",
    "    ('combined_livingspace', DFTransform(lambda X: combined_livingspace(X)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dummies...\n",
      "Starting with input of shape: (2915, 79)\n",
      "Returning output of shape: (2915, 221)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "train_df =  pd.read_csv('data/train.csv')\n",
    "X_train = train_df.drop(['SalePrice','Id'], axis=1)\n",
    "y_train = train_df['SalePrice']\n",
    "X_test = pd.read_csv('data/test.csv').drop(['Id'], axis=1)\n",
    "X_train, y_train = prepare_inputs(X_train, y_train)\n",
    "\n",
    "# Transforming the input\n",
    "X_combined = pd.concat((X_train, X_test)).reset_index(drop=True) \n",
    "X_tranformed = trans_pipeline.fit_transform(X_combined)\n",
    "\n",
    "# Split the transformed input back\n",
    "X_train_trans = X_tranformed[:X_train.shape[0]] \n",
    "X_test_trans = X_tranformed[X_train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_trans, y_train, test_size=.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Define first estimator\n",
    "adaboost_estimator = AdaBoostRegressor(base_estimator=Ridge(alpha=10, copy_X=True, fit_intercept=True, max_iter=None,\n",
    "   normalize=False, random_state=42, solver='cholesky', tol=0.001),\n",
    "         learning_rate=0.0001, loss='square', n_estimators=500,\n",
    "         random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostRegressor(base_estimator=Ridge(alpha=10, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=42, solver='cholesky', tol=0.001),\n",
       "         learning_rate=0.0001, loss='square', n_estimators=500,\n",
       "         random_state=42)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the first estimator on the first subset\n",
    "adaboost_estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict with the first estimator on the second subset\n",
    "prediction_adaboost = adaboost_estimator.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Define second regressor\n",
    "lasso_estimator = BaggingRegressor(\n",
    "        Pipeline(memory=None, steps=[('lasso_regression', Lasso(alpha=0.0001, copy_X=True, fit_intercept=True, max_iter=1000,\n",
    "   normalize=True, positive=False, precompute=False, random_state=42,\n",
    "   selection='cyclic', tol=0.0001, warm_start=False))]), \n",
    "        n_estimators=500, bootstrap=True, n_jobs=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingRegressor(base_estimator=Pipeline(memory=None,\n",
       "     steps=[('lasso_regression', Lasso(alpha=0.0001, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=True, positive=False, precompute=False, random_state=42,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False))]),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=1.0, n_estimators=500, n_jobs=4, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the second estimator on the first subset\n",
    "lasso_estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict with the second estimator on the second subset\n",
    "prediction_lasso = lasso_estimator.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('robust_scaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
       "       with_scaling=True)), ('ridge_regression', Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='cholesky', tol=0.001))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'robust_scaler__with_scaling': [True, False], 'robust_scaler__with_centering': [True, False], 'ridge_regression__alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=make_scorer(rmse_score, greater_is_better=False), verbose=0)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.model_selection import GridSearchCV   \n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "param_dict = {'robust_scaler__with_scaling' : [True, False],\n",
    "              'robust_scaler__with_centering' : [True, False],\n",
    "              'ridge_regression__alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "             }\n",
    "     \n",
    "scorer = make_scorer(rmse_score, greater_is_better=False)\n",
    "parameter_estimator = GridSearchCV(Pipeline([\n",
    "        ('robust_scaler', RobustScaler()),\n",
    "        ('ridge_regression', Ridge(solver='cholesky'))\n",
    "            ]), param_dict, scoring=scorer)\n",
    "\n",
    "parameter_estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not we define a bagging ensemble for the blending\n",
    "bagging_estimator = parameter_estimator.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('robust_scaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=False,\n",
       "       with_scaling=True)), ('ridge_regression', Ridge(alpha=10, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='cholesky', tol=0.001))])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This blender now we train on the predictions of the first layer\n",
    "X_blended = np.column_stack((prediction_lasso, prediction_adaboost))\n",
    "\n",
    "bagging_estimator.fit(X_blended, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And now we use the stack to make a prediction on unseen data\n",
    "test_prediction_ada = adaboost_estimator.predict(X_test)\n",
    "test_prediction_lasso = lasso_estimator.predict(X_test)\n",
    "\n",
    "X_test_blended = np.column_stack((test_prediction_lasso, test_prediction_ada))\n",
    "y_predicted = bagging_estimator.predict(X_test_blended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2-score: 0.904038380484\n",
      "RMSE (log): 0.12244306131555652\n"
     ]
    }
   ],
   "source": [
    "print_benchmark(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dummies...\n",
      "Starting with input of shape: (2915, 79)\n",
      "Returning output of shape: (2915, 221)\n"
     ]
    }
   ],
   "source": [
    "# Now we train and execute the stacked model on complete data set\n",
    "train_df =  pd.read_csv('data/train.csv')\n",
    "X_train = train_df.drop(['SalePrice','Id'], axis=1)\n",
    "y_train = train_df['SalePrice']\n",
    "X_test = pd.read_csv('data/test.csv').drop(['Id'], axis=1)\n",
    "X_train, y_train = prepare_inputs(X_train, y_train)\n",
    "\n",
    "# Transforming the input\n",
    "X_combined = pd.concat((X_train, X_test)).reset_index(drop=True) \n",
    "X_tranformed = trans_pipeline.fit_transform(X_combined)\n",
    "\n",
    "# Split the transformed input back\n",
    "X_train_trans = X_tranformed[:X_train.shape[0]] \n",
    "X_test_trans = X_tranformed[X_train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dummies...\n",
      "Starting with input of shape: (2915, 79)\n",
      "Returning output of shape: (2915, 221)\n"
     ]
    }
   ],
   "source": [
    "# First we train on all the training data\n",
    "train_df =  pd.read_csv('data/train.csv')\n",
    "X_train = train_df.drop(['SalePrice','Id'], axis=1)\n",
    "y_train = train_df['SalePrice']\n",
    "X_test = pd.read_csv('data/test.csv').drop(['Id'], axis=1)\n",
    "X_train, y_train = prepare_inputs(X_train, y_train)\n",
    "\n",
    "# Transforming the input\n",
    "X_combined = pd.concat((X_train, X_test)).reset_index(drop=True) \n",
    "X_tranformed = trans_pipeline.fit_transform(X_combined)\n",
    "\n",
    "# Split the transformed input back\n",
    "X_train_trans = X_tranformed[:X_train.shape[0]] \n",
    "X_test_trans = X_tranformed[X_train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('robust_scaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=False,\n",
       "       with_scaling=True)), ('ridge_regression', Ridge(alpha=10, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='cholesky', tol=0.001))])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we fit the layer 1 estimators to all the training data\n",
    "adaboost_estimator.fit(X_train_trans, y_train)\n",
    "lasso_estimator.fit(X_train_trans, y_train)\n",
    "\n",
    "# We also train the blender on all the training data\n",
    "predictions_train_ada = adaboost_estimator.predict(X_train_trans)\n",
    "predictions_train_lasso = lasso_estimator.predict(X_train_trans)\n",
    "X_train_blended = np.column_stack((predictions_train_lasso, predictions_train_ada))\n",
    "bagging_estimator.fit(X_train_blended, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2-score: 0.933802274265\n",
      "RMSE (log): 0.1018713919828518\n"
     ]
    }
   ],
   "source": [
    "# To check we first evaluate the predictions on the whole test set\n",
    "y_predicted_test = bagging_estimator.predict(X_train_blended)\n",
    "print_benchmark(y_train, y_predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we predict on the test set\n",
    "\n",
    "# Make predictions in the first layer\n",
    "predictions_test_ada = adaboost_estimator.predict(X_test_trans)\n",
    "predictions_test_lasso = lasso_estimator.predict(X_test_trans)\n",
    "\n",
    "# And put them in the blender\n",
    "X_test_blended = np.column_stack((test_prediction_lasso, test_prediction_ada))\n",
    "y_predicted = bagging_estimator.predict(X_test_blended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written to C:\\Source\\predicting-house-prices\\submissions\\20180823091142.csv\n"
     ]
    }
   ],
   "source": [
    "write_submission(y_predicted, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle score: 0.12111"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
